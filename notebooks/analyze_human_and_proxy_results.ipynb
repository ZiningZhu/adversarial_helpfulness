{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze human annotation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from pathlib import Path \n",
    "import scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 42)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['HITId', 'HITTypeId', 'Title', 'Description', 'Keywords', 'Reward',\n",
       "       'CreationTime', 'MaxAssignments', 'RequesterAnnotation',\n",
       "       'AssignmentDurationInSeconds', 'AutoApprovalDelayInSeconds',\n",
       "       'Expiration', 'NumberOfSimilarHITs', 'LifetimeInSeconds',\n",
       "       'AssignmentId', 'WorkerId', 'AssignmentStatus', 'AcceptTime',\n",
       "       'SubmitTime', 'AutoApprovalTime', 'ApprovalTime', 'RejectionTime',\n",
       "       'RequesterFeedback', 'WorkTimeInSeconds', 'LifetimeApprovalRate',\n",
       "       'Last30DaysApprovalRate', 'Last7DaysApprovalRate', 'Input.ECQA_Number',\n",
       "       'Input.question', 'Input.choice_A', 'Input.choice_B', 'Input.choice_C',\n",
       "       'Input.choice_D', 'Input.choice_E', 'Input.answer', 'Input.explanation',\n",
       "       'Answer.convincingness_after', 'Answer.convincingness_before',\n",
       "       'Answer.correctness', 'Answer.fluency', 'Approve', 'Reject'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ECQA_SecondBest_method_noS_500_annotated.csv\")\n",
    "print(df.shape)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 500\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 2.9413, sd 1.0942\n",
      "\tAnswer.convincingness_after:\t mean 3.6080, sd 1.0156\n",
      "\tAnswer.fluency:\t mean 4.8613, sd 0.3355\n",
      "\tAnswer.correctness:\t mean 4.5240, sd 0.5895\n",
      "t_test: convincingness before vs after\t< ***\n"
     ]
    }
   ],
   "source": [
    "def p_to_stars(pval, bonferroni=1):\n",
    "    if pval < 0.001/bonferroni:\n",
    "        return \"***\"\n",
    "    elif pval < 0.01/bonferroni:\n",
    "        return \"**\"\n",
    "    elif pval < 0.05/bonferroni:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def print_reports(df, bonferroni=3):\n",
    "    print(\"Number of questions:\", len(df.HITId.unique()))\n",
    "    \n",
    "    # Average score of each question.\n",
    "    print(\"Average scores of each question:\")\n",
    "    scores = [\"Answer.convincingness_before\", \"Answer.convincingness_after\", \"Answer.fluency\", \"Answer.correctness\"]\n",
    "    df_mean = df[[\"HITId\"] + scores].groupby(\"HITId\").mean()\n",
    "    \n",
    "    for sc in scores:\n",
    "        print(\"\\t{}:\\t mean {:.4f}, sd {:.4f}\".format(sc, df_mean[sc].mean(), df_mean[sc].std()))\n",
    "\n",
    "    # T test\n",
    "    print(\"t_test: convincingness before vs after\", end=\"\\t\")\n",
    "    s, pval = scipy.stats.ttest_rel(df_mean[\"Answer.convincingness_before\"], df_mean[\"Answer.convincingness_after\"])\n",
    "    sign = \"<\" if s < 0 else \">\"\n",
    "    stars = p_to_stars(pval, bonferroni=bonferroni)\n",
    "    print(\"{} {}\".format(sign, stars))\n",
    "\n",
    "print_reports(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 500\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 2.9560, sd 0.9866\n",
      "\tAnswer.convincingness_after:\t mean 3.5267, sd 0.9261\n",
      "\tAnswer.fluency:\t mean 4.8520, sd 0.3459\n",
      "\tAnswer.correctness:\t mean 4.6800, sd 0.5210\n",
      "t_test: convincingness before vs after\t< ***\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ECQA_SecondBest_method_S_500_annotated.csv\")\n",
    "print_reports(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proxy evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model=mixtral-8x7B, task=ECQA_SecondBest_method_noS_500\n",
      "Number of questions: 500\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 2.6080, sd 0.9840\n",
      "\tAnswer.convincingness_after:\t mean 2.5880, sd 0.8097\n",
      "\tAnswer.fluency:\t mean 1.9520, sd 0.9998\n",
      "\tAnswer.correctness:\t mean 2.9760, sd 0.2180\n",
      "t_test: convincingness before vs after\t> \n",
      "\n",
      "model=mixtral-8x7B, task=ECQA_SecondBest_method_S_500\n",
      "Number of questions: 500\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 2.6080, sd 0.9840\n",
      "\tAnswer.convincingness_after:\t mean 2.5880, sd 0.8097\n",
      "\tAnswer.fluency:\t mean 1.9520, sd 0.9998\n",
      "\tAnswer.correctness:\t mean 2.9760, sd 0.2180\n",
      "t_test: convincingness before vs after\t> \n",
      "\n",
      "model=mixtral-8x7B, task=NLI_entail_to_neutral\n",
      "Number of questions: 300\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 2.7667, sd 0.7028\n",
      "\tAnswer.convincingness_after:\t mean 2.9933, sd 0.1155\n",
      "\tAnswer.fluency:\t mean 1.7800, sd 0.9771\n",
      "\tAnswer.correctness:\t mean 2.9467, sd 0.3228\n",
      "t_test: convincingness before vs after\t< ***\n",
      "\n",
      "model=vicuna-33B, task=ECQA_SecondBest_method_noS_500\n",
      "Number of questions: 500\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 1.6400, sd 1.2302\n",
      "\tAnswer.convincingness_after:\t mean 3.0120, sd 0.1546\n",
      "\tAnswer.fluency:\t mean 1.3040, sd 0.7407\n",
      "\tAnswer.correctness:\t mean 1.3880, sd 0.9139\n",
      "t_test: convincingness before vs after\t< ***\n",
      "\n",
      "model=vicuna-33B, task=ECQA_SecondBest_method_S_500\n",
      "Number of questions: 500\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 1.6400, sd 1.2302\n",
      "\tAnswer.convincingness_after:\t mean 3.0120, sd 0.1546\n",
      "\tAnswer.fluency:\t mean 1.3040, sd 0.7407\n",
      "\tAnswer.correctness:\t mean 1.3880, sd 0.9139\n",
      "t_test: convincingness before vs after\t< ***\n",
      "\n",
      "model=vicuna-33B, task=NLI_entail_to_neutral\n",
      "Number of questions: 300\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 1.1133, sd 0.4912\n",
      "\tAnswer.convincingness_after:\t mean 3.0000, sd 0.0000\n",
      "\tAnswer.fluency:\t mean 1.1067, sd 0.4501\n",
      "\tAnswer.correctness:\t mean 1.1200, sd 0.5290\n",
      "t_test: convincingness before vs after\t< ***\n"
     ]
    }
   ],
   "source": [
    "models = [\"mixtral-8x7B\", \"vicuna-33B\"]\n",
    "tasks = [\n",
    "    \"ECQA_SecondBest_method_noS_500\",\n",
    "    \"ECQA_SecondBest_method_S_500\",\n",
    "    \"NLI_entail_to_neutral\"\n",
    "]\n",
    "for model in models:\n",
    "    for task in tasks:\n",
    "        print(f\"\\nmodel={model}, task={task}\")\n",
    "        df = pd.read_csv(f\"../data/{model}/{task}.csv\")\\\n",
    "            .rename(columns={\"convincingness_before\": \"Answer.convincingness_before\",\n",
    "                    \"convincingness_after\": \"Answer.convincingness_after\",\n",
    "                    \"fluency\": \"Answer.fluency\",\n",
    "                    \"correctness\": \"Answer.correctness\"})\n",
    "        df[\"HITId\"] = list(range(len(df)))\n",
    "        print_reports(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeledit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
