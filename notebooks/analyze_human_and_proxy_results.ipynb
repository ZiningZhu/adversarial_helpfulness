{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze human annotation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from pathlib import Path \n",
    "from scipy.stats import ttest_rel, ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['HITId', 'HITTypeId', 'Title', 'Description', 'Keywords', 'Reward',\n",
       "       'CreationTime', 'MaxAssignments', 'RequesterAnnotation',\n",
       "       'AssignmentDurationInSeconds', 'AutoApprovalDelayInSeconds',\n",
       "       'Expiration', 'NumberOfSimilarHITs', 'LifetimeInSeconds',\n",
       "       'AssignmentId', 'WorkerId', 'AssignmentStatus', 'AcceptTime',\n",
       "       'SubmitTime', 'AutoApprovalTime', 'ApprovalTime', 'RejectionTime',\n",
       "       'RequesterFeedback', 'WorkTimeInSeconds', 'LifetimeApprovalRate',\n",
       "       'Last30DaysApprovalRate', 'Last7DaysApprovalRate', 'Input.question',\n",
       "       'Input.choice_A', 'Input.choice_B', 'Input.choice_C', 'Input.choice_D',\n",
       "       'Input.choice_E', 'Input.answer', 'Input.explanation',\n",
       "       'Answer.convincingness_after', 'Answer.convincingness_before',\n",
       "       'Answer.correctness', 'Answer.fluency', 'Approve', 'Reject'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ecqa/secondbest/gpt4/with_nle_annotated.csv\")\n",
    "print(df.shape)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 500\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 2.9560, sd 0.9866\n",
      "\tAnswer.convincingness_after:\t mean 3.5267, sd 0.9261\n",
      "\tAnswer.fluency:\t mean 4.8520, sd 0.3459\n",
      "\tAnswer.correctness:\t mean 4.6800, sd 0.5210\n",
      "t_test: convincingness before vs after\t< ***\n"
     ]
    }
   ],
   "source": [
    "def p_to_stars(pval, bonferroni=1):\n",
    "    if pval < 0.001/bonferroni:\n",
    "        return \"***\"\n",
    "    elif pval < 0.01/bonferroni:\n",
    "        return \"**\"\n",
    "    elif pval < 0.05/bonferroni:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def print_reports(df, bonferroni=3):\n",
    "    print(\"Number of questions:\", len(df.HITId.unique()))\n",
    "    \n",
    "    # Average score of each question.\n",
    "    print(\"Average scores of each question:\")\n",
    "    scores = [\"Answer.convincingness_before\", \"Answer.convincingness_after\", \"Answer.fluency\", \"Answer.correctness\"]\n",
    "    df_mean = df[[\"HITId\"] + scores].groupby(\"HITId\").mean()\n",
    "    \n",
    "    for sc in scores:\n",
    "        print(\"\\t{}:\\t mean {:.4f}, sd {:.4f}\".format(sc, df_mean[sc].mean(), df_mean[sc].std()))\n",
    "\n",
    "    # T test\n",
    "    print(\"t_test: convincingness before vs after\", end=\"\\t\")\n",
    "    s, pval = ttest_rel(df_mean[\"Answer.convincingness_before\"], df_mean[\"Answer.convincingness_after\"])\n",
    "    sign = \"<\" if s < 0 else \">\"\n",
    "    stars = p_to_stars(pval, bonferroni=bonferroni)\n",
    "    print(\"{} {}\".format(sign, stars))\n",
    "\n",
    "print_reports(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proxy evaluation results\n",
    "\n",
    "Up till now, the mixtral-8x7B results seem a bit weird. Vicuna-33B shows results that correlate to humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "explainer=gpt4, task=ecqa/secondbest, proxy=mixtral-8x7B\n",
      "Number of questions: 500\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 2.6080, sd 0.9840\n",
      "\tAnswer.convincingness_after:\t mean 2.5880, sd 0.8097\n",
      "\tAnswer.fluency:\t mean 1.9520, sd 0.9998\n",
      "\tAnswer.correctness:\t mean 2.9760, sd 0.2180\n",
      "t_test: convincingness before vs after\t> \n",
      "\n",
      "explainer=gpt4, task=ecqa/secondbest, proxy=vicuna-33B\n",
      "Number of questions: 500\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 1.6400, sd 1.2302\n",
      "\tAnswer.convincingness_after:\t mean 3.0120, sd 0.1546\n",
      "\tAnswer.fluency:\t mean 1.3040, sd 0.7407\n",
      "\tAnswer.correctness:\t mean 1.3880, sd 0.9139\n",
      "t_test: convincingness before vs after\t< ***\n",
      "\n",
      "explainer=gpt4, task=nli/entail_to_neutral, proxy=mixtral-8x7B\n",
      "Number of questions: 300\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 2.9933, sd 0.3469\n",
      "\tAnswer.convincingness_after:\t mean 3.0000, sd 0.0000\n",
      "\tAnswer.fluency:\t mean 2.2133, sd 0.9786\n",
      "\tAnswer.correctness:\t mean 2.9867, sd 0.1630\n",
      "t_test: convincingness before vs after\t< \n",
      "\n",
      "explainer=gpt4, task=nli/entail_to_neutral, proxy=vicuna-33B\n",
      "Number of questions: 300\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 1.1333, sd 0.5507\n",
      "\tAnswer.convincingness_after:\t mean 3.0000, sd 0.0000\n",
      "\tAnswer.fluency:\t mean 1.1133, sd 0.4912\n",
      "\tAnswer.correctness:\t mean 1.1133, sd 0.4912\n",
      "t_test: convincingness before vs after\t< ***\n",
      "\n",
      "explainer=gpt4, task=nli/contra_to_neutral, proxy=mixtral-8x7B\n",
      "Number of questions: 300\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 3.0133, sd 0.3655\n",
      "\tAnswer.convincingness_after:\t mean 3.0000, sd 0.0000\n",
      "\tAnswer.fluency:\t mean 2.0400, sd 1.0009\n",
      "\tAnswer.correctness:\t mean 3.0000, sd 0.0000\n",
      "t_test: convincingness before vs after\t> \n",
      "\n",
      "explainer=gpt4, task=nli/contra_to_neutral, proxy=vicuna-33B\n",
      "Number of questions: 300\n",
      "Average scores of each question:\n",
      "\tAnswer.convincingness_before:\t mean 1.1533, sd 0.6254\n",
      "\tAnswer.convincingness_after:\t mean 3.0000, sd 0.0000\n",
      "\tAnswer.fluency:\t mean 1.1000, sd 0.4366\n",
      "\tAnswer.correctness:\t mean 1.1133, sd 0.5177\n",
      "t_test: convincingness before vs after\t< ***\n"
     ]
    }
   ],
   "source": [
    "explainers = [\"gpt4\"]\n",
    "proxy_models = [\"mixtral-8x7B\", \"vicuna-33B\"]\n",
    "tasks = [\n",
    "    \"ecqa/secondbest\",\n",
    "    \"nli/entail_to_neutral\",\n",
    "    \"nli/contra_to_neutral\"\n",
    "]\n",
    "for explainer in explainers:\n",
    "    for task in tasks:\n",
    "        for proxy in proxy_models:\n",
    "            print(f\"\\nexplainer={explainer}, task={task}, proxy={proxy}\")\n",
    "            \n",
    "            df = pd.read_csv(f\"../data/{task}/{explainer}/with_nle_scored_by_{proxy}.csv\")\\\n",
    "                .rename(columns={\"convincingness_before\": \"Answer.convincingness_before\",\n",
    "                        \"convincingness_after\": \"Answer.convincingness_after\",\n",
    "                        \"fluency\": \"Answer.fluency\",\n",
    "                        \"correctness\": \"Answer.correctness\"})\n",
    "            df[\"HITId\"] = list(range(len(df)))  # Add this column so the print_reports(), which averages by HITId, can work\n",
    "            print_reports(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeledit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
